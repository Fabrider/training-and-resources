Glossary I
There is no need to either have strong data skills, or learn new data skills for this workshop. However, we might (on purpose or out of habit) use terms and concepts that have different meanings than what you’re used to, or might be entirely new. For this reason, we decided to provide two glossaries to help you prepare for the Expedition.
This first glossary will tackle some concepts which might seem obvious, and perhaps not related to our expedition. However, the next glossary will deal with terms and concepts more directly related to a fully fledged Data Expedition. 
We’ve tried to collate concepts that technologists are most likely to throw around as if common knowledge, and we aim to demystify them. Bear in mind that this glossary list is neither comprehensive nor definitive. It is our way of creating a common ground around terms and concepts that we will be using together.
________________


Algorithm
This term is carelessly thrown around very often, and it gained notoriety and a level of almost mystic importance in today’s world. The strict definition of an algorithm is a process or set of rules to be followed: basically, a recipe. And in fact, a recipe is an algorithm: by following the steps of a recipe, you will achieve your goal of having a cake. Algorithms aren’t code, or a programming language: algorithms are ways to think about achieving a certain goal by defining which steps are needed to go from A (I want a cake)  to B (I have a cake). 


Data
Many times when people think of data, their minds automatically goes to spreadsheets, databases, or lists of numbers. In the most general sense, data is a value assigned to a thing. But, the thing itself is also data. Data, in essence, is a collection of values. 
When using data, technologists often implicitly refer to any type of value that can be collected about a thing. This is true of physical things (the number of chairs in the room you’re sitting in is data, as is the color of those chairs, or their occupancy), as well as digital things (numbers in a spreadsheet, the number of your Facebook friends).
It’s useful to think of data in the following way: in order to gain knowledge about something, you need information, which can be gathered by analysing data. So data is the raw material on the road to knowledge.
Data types
Data types are qualities, or categories, of data, that tell you how that data can be used. In the digital world, data types are defined by which operations a computer can perform on that data. 
Computers have an extremely limited set of operations they can do with data: their strength lies in the fact they can do those operations billions of times per second. Computers understand only a couple of very basic data types: numbers (34, -2, 0.005), characters (a, t, z) and strings (“James”, “apple”), and true/false statements. There is much more to be unpacked around how computers deal with data types, but this makes for a good baseline. 
It is important to realize that the data type makes certain operations possible — or impossible. For example, in a spreadsheet, if an entire column holds only number data types, that means we can ask our spreadsheet application to perform math on those columns: averages, sums, percentages etc. If another column only holds character strings, we can ask the computer to sort them alphabetically, or compare them by length. 
This is what data cleaning is, essentially: making sure data is in types we can use to extract information. For example, if we have an “age” column in a spreadsheet, we will need all cells to have only numbers. A cell that holds the string “fortyseven” doesn’t have the same numeric significance for the computer as it does to our eyes, so we would need to change it to “47” in order to perform spreadsheet magic.
Data structure
Another important concept is unstructured versus structured data. The difference between the two is the categorisation and standardisation of the collected data. The more structured a data collection is, the easier it is to gather information about the data. 
For example, imagine a spreadsheet of names, surnames, and numbers between 18 and 74, all randomly placed in cells. This is data — but being unstructured, it is almost impossible to gain meaningful information from it. 
Now imagine the same spreadsheet, but with names, surnames, and numbers each in a separate column — and each column with a  header: “name” “surname” and “age”. This structuring of the same exact data lets you understand it more easily, and gain knowledge about, for example, how many people in their thirties have a surname that starts with “J”.
File formats
There are different ways of structuring and collecting information — for an example in digital data, software applications store data inside files, like an .xls file for Excel documents. That “.xls” part of the file name tells us the format in which the data is stored. In this case, we know that data will be presented in a spreadsheet, that can be opened with Microsoft Excel. 
When talking about usability of data, there are “good” and “bad” formats  — for example, a PDF of a table will generally let you look at the data, but won’t let you modify it or analyse it with tools other than your own eyes. 
Machine-readable data
Machine-readable means it is possible for a computer to understand the data by itself. Now, computers will always understand that a digital file holds some sort of data: the question is whether it understands that data the way we want it to. For example, a digital image (let’s say a “.jpg”) of the number 7 is humanly legible (because our eyes can see the data and our brains can turn them into information), but a computer wouldn’t be able to understand it in the same way. In fact, nothing in that .jpg is telling the computer “that is a 7”: the only thing a computer knows is “there are pixels of different colors in that file”. A good way of knowing whether data is machine-readable is by asking the machine to perform an operation on a file that requires understanding the content, like alphabetical sorting of a spreadsheet column, or calculating an average. 
The importance of making our data machine-readable is to reduce manual work as much as possible: if computers can understand our data by themselves, there’s no need for humans to get involved and we can have computers work on enormous amounts of data for us. You might have used the mobile application “Citymapper”: it helps you find the best public transport options to get you from one place to another. Citymapper relies on cities sharing open data about public transport, like real time updates on bus and train routes. Citymapper also relies on that data being machine-readable to the point of full automation: there is no way humans would be able to manually comprehend  and analyse public transport data of an entire city in real time.
Metadata
Metadata is “data about data”. It is the information we can gain about a data collection as a whole. For example, the name of a file is metadata: what we know about the file is its name. A file format is also metadata. 
A good example is a picture taken with a mobile phone: today’s mobile devices store all sorts of metadata together with the image that is being taken. Besides the filename, a mobile device can collect information about the place where the photo was taken (GPS coordinates), time and date, image resolution, etc. Another good example is Twitter: a single tweet contains more than 40 types of information (metadata) about the tweet itself.
Open Data
Open Data is a cornerstone of our vision around accessible information. In its most succinct definition, Open Data is data that can be freely used, reused and redistributed by anyone - subject only, at most, to the requirement to attribute and sharealike.
Now there are lots of details to unpack around how open data is created and what should, or shouldn’t, be open — but in the broadest sense, open data defines both the state (data that is open) and the purpose (data that is meant to be reused openly). It is also both a practical and an ideological approach to working with data. As you’ve seen in this glossary, issues around the proprietary nature of file formats, structured and machine readable data, all contribute to openness of a dataset.
Data can be partially open or partially closed — the definitive nature depends a lot on where you stand on the range between pragmatism and belief.
Proprietary
Proprietary in computer speak means “belonging to a corporate entity”. You might have heard people mention proprietary software (like Adobe Acrobat), or proprietary file formats (like “.doc”). The reason why technologists often talk about proprietary in negative terms is because it might limit the openness of data that is being stored and used within those formats. 
For example, PDF files can be locked so that no other application is able to open them. Another problem with proprietary is data longevity: there is no guarantee that, once the corporation that produces a certain application decides to stop developing it, the data stored in that application’s file system will be usable and readable by other applications. This puts data at risk of becoming irretrievable.
More resources
* The Open Data handbook and glossary: http://opendatahandbook.org/glossary/en/